{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts and terminology\n",
    "\n",
    "This checkpoint will start by introducing some basic concepts and terminology that are used frequently in the NLP domain. Get familiar with these terms, because they will be used often in the following checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "\n",
    "A *corpus* (plural: *corpora*) is the collection of texts that you work on. So, if you're analyzing all the news published in *The New York Times* in 2018, then your corpus is the collection of all that news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document\n",
    "\n",
    "In NLP, the use of the term *document* slightly differs from that of spoken English, and it might be confusing when you see this term in the NLP context. In NLP, the basic unit of observation is called a *document*. Recall that in the previous modules, you worked with tabular data where each row represents an observation and each column represents a feature. When working with text data, the observations are often referred to as documents. \n",
    "\n",
    "For example, say that you have a bunch of articles written by several authors, and your task is to predict the author of a given article. In this case, *document* refers to the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary\n",
    "\n",
    "*Vocabulary* refers to the list of all unique words in a corpus. Note that the vocabulary depends on the corpus at hand. So, for different corpora, there exist different vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop words\n",
    "\n",
    "Words that are frequently encountered but that hold relatively unimportant meanings are called *stop words*. As you'll see in upcoming checkpoints, you'll typically remove the stop words from your dataset when you process data. Note that different languages naturally have different stop words. So when you want to eliminate the stop words from your dataset, you need to be careful about using the correct set of stop words for that specific language. \n",
    "\n",
    "Although there's no complete list of stop words, here are some English ones:\n",
    "\n",
    "i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n",
    "\n",
    "Although stop words are often removed from the data during the text preprocessing stage, that isn't always the case. In some NLP problems, it might make sense to retain them, because stop words can make up part of meaningful phrases. For example, `master of the universe` is more specific and informative than `master` and `universe` alone. Yet, in many applications, you may prefer to remove them before analyzing your text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token and tokenization\n",
    "\n",
    "In an NLP application, two of the most common concepts that you'll come across are *token* and *tokenization*. Each individual meaningful piece in a document is a *token*, and the process of breaking up the document into these pieces is called *tokenization*. \n",
    "\n",
    "Tokens are generally words and punctuation marks. You may discard some tokens that you don't think add informational value. As mentioned before, one class of potentially uninformative tokens are stop words. Punctuation marks are also commonly discarded.\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "The tokens in this sentence are `I`, `prefer`, `to`, `study`, `natural`, `language`, `processing`, `instead`, `of`, `watching`, `game`, `of`, and `thrones`. If you want to keep punctuation in your work, then dot `.` is also a token for the sentence above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lemma\n",
    "\n",
    "Loosely speaking, a word's root is called its *lemma*. The following [quote from Wikipedia](https://en.wikipedia.org/wiki/Lemma_(morphology) provides a more concrete definition:\n",
    "\n",
    "> \"In morphology and lexicography, a lemma (plural: *lemmas* or *lemmata*) is the canonical form, dictionary form, or citation form of a set of words (headword). In English, for example, *run*, *runs*, *ran*, and *running* are forms of the same *lexeme*, with *run* as the lemma. *Lexeme*, in this context, refers to the set of all the forms that have the same meaning, and *lemma* refers to the particular form that is chosen by convention to represent the lexeme. In lexicography, this unit is usually also the citation form or headword by which it is indexed.\"\n",
    "\n",
    "The process of determining the lemmas of the tokens is called *lemmatization*. Now, try to determine the lemmas of the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "| Token | Lemma |\n",
    "| --- | --- |\n",
    "| I | I |\n",
    "| prefer | prefer |\n",
    "| to | to |\n",
    "| study | study |\n",
    "| natural | natural |\n",
    "| language | language |\n",
    "| processing | processing |\n",
    "| instead | instead |\n",
    "| of | of |\n",
    "| watching | watch |\n",
    "| Game | game |\n",
    "| of | of |\n",
    "| Thrones | thrones |\n",
    "| . | . |\n",
    "\n",
    "You may be surprised that the lemmas of all the tokens except `watching` are the same as their original forms. Moreover, the lemma of `watching` is `watch`, but the lemma of `processing` is `processing`. That's because the `-ing` suffix makes `processing` a noun, where it makes `watching` an inflected form of `watch`. So the lemmatization here takes that difference into consideration. That being said, keep in mind that there are several ways of lemmatizing a token. Hence, you may encounter different lemmas for the same word, depending on the lemmatizer that you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stem\n",
    "\n",
    "Another concept that's close to lemma is the *stem*. Stem refers to the core part of a word that never changes, even in the different forms of that word. Here's the definition [according to Wikipedia](https://en.wikipedia.org/wiki/Lemma_(morphology):\n",
    "\n",
    "> \"The stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. For example, from `produced`, the lemma is `produce`, but the stem is `produc-`. This is because there are words such as production. In linguistic analysis, the stem is defined more generally as the analyzed base form from which all inflected forms can be formed.\"\n",
    "\n",
    "Determining the stems of the tokens is known as *stemming*. You can think of stems as a more distilled form of lemmas. When lemmatizing, your goal is to retain each word's core meaning; you want to find the meaning that is shared among a word's different forms. When stemming, your goal is also to retain the core meaning. But with stemming, you are trying to do this by retaining only the characters that convey the same meaning in all of the forms of a word.\n",
    "\n",
    "Now, derive the stems of the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "| Token | Stem |\n",
    "| --- | --- |\n",
    "| I | I |\n",
    "| prefer | prefer |\n",
    "| to | to |\n",
    "| study | studi |\n",
    "| natural | natur |\n",
    "| language | languag |\n",
    "| processing | process |\n",
    "| instead | instead |\n",
    "| of | of |\n",
    "| watching | watch |\n",
    "| Game | game |\n",
    "| of | of |\n",
    "| Thrones | throne |\n",
    "| . | . |\n",
    "\n",
    "As you can see, some stems seem strange, and you may not be able to find them in an English dictionary. That is because a stemmer's job is to turn inflected forms of tokens into some common root, without considering whether or not that root is a \"proper\" word that can be found in a dictionary. Also note that, as with lemmatizers, different stemmers exist, and they may produce different stems for the same token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Part of speech\n",
    "\n",
    "*Part of speech* (POS) refers to the particular class of words in a language. The words in the same class usually play a similar role in a sentence. Here's the definition [according to Wikipedia](https://en.wikipedia.org/wiki/Part_of_speech):\n",
    "\n",
    "> \"In traditional grammar, a part of speech (abbreviated form: PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties. Words that are assigned to the same part of speech generally display similar behavior in terms of syntax—they play similar roles within the grammatical structure of sentences—and sometimes in terms of morphology, in that they undergo inflection for similar properties ... Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article, or determiner.\"\n",
    "\n",
    "Determining a word's part of speech is especially useful for distinguishing between *homographs*, which are words with the same spelling but different meanings. (The umbrella term for this kind of linguistic feature is *polysemy*.) For example, the word `break` is a noun in \"I need a break\" but a verb in \"I need to break the glass\".\n",
    "\n",
    "As an example, consider the following sentence again:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "The part of speech of the words of this sentence are as follows:\n",
    "\n",
    "| Token | POS tag |\n",
    "| --- | --- |\n",
    "| I | PRP |\n",
    "| prefer | VBP |\n",
    "| to | TO |\n",
    "| study | VB |\n",
    "| natural | JJ |\n",
    "| language | NN |\n",
    "| processing | VBG |\n",
    "| instead | RB |\n",
    "| of | IN |\n",
    "| watching | VBG |\n",
    "| Game | NNP |\n",
    "| of | IN |\n",
    "| Thrones | NNP |\n",
    "| . | . |\n",
    "\n",
    "In the example above, PRP refers to the pronoun or personal, VBP refers to the present-tense verb but not the third-person singular form, and so forth. The details of each tag won't be covered here, but the tools that you'll use to derive the sentences' POS tags have documentation that explains these tags in detail. Note that several POS taggers exist, and each has its own terminology of POS tag names.\n",
    "\n",
    "Lastly, note that different POS taggers may produce different results for the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Named entity\n",
    "\n",
    "*Named entities* are the words (or group of words) in a sentence that indicate a predefined category of objects (*entities*). These categories include but are not limited to person names, organizations, locations, time expressions, quantities, monetary values, and percentages. The task of determining the named entities in a sentence is known as *named entity recognition* (NER).\n",
    "\n",
    "Consider the following sentence as an example:\n",
    "\n",
    "    The Federal Reserve decided to decrease the interest rates by 0.25% in their last FOMC meeting.\n",
    "    \n",
    "Here, `Federal Reserve` is the central bank of the US; hence, it's an organization. And `0.25%` is a percentage. Be cautious here as well; there are several named entity parsers, and they may produce different results for the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dependency graph\n",
    "\n",
    "The *dependency graph* (also known as *dependency tree*) of a sentence represents the grammatical structure and indicates the relationship between the words. These graphs are often represented as a tree structure where the top word is called the *root* or *head*. Dependency graphs are useful for understanding how words relate to each other syntactically. The task of building a dependency graph is known as *dependency parsing*.\n",
    "\n",
    "Dependencies are a bit complicated. For a visual example of dependencies expressed as a tree, check the [About section of the Stanford NLP Group Dependencies page](https://nlp.stanford.edu/software/stanford-dependencies.shtml). Stanford's NLP group has had a lot of influence in this field, so you're likely to run across them frequently if you go deep into NLP. This program won't cover different methods of deriving the dependency graphs, but here's a visual representation of the dependency graph for this sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "![dependency_graph.png](assets/dependency_graph.png)\n",
    "\n",
    "Again, note that several dependency parsers exist, and each may result in different dependency graphs for the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main NLP packages in the Python ecosystem\n",
    "\n",
    "In the Python ecosystem, there are several NLP packages that you can use in your work. And due to the fast pace of advances in the NLP field, new packages are being added quite frequently. However, in this module, you'll use the most established packages. Specifically, you'll use the following:\n",
    "\n",
    "* Scikit-learn\n",
    "\n",
    "* Natural Language Toolkit (NLTK)\n",
    "\n",
    "* spaCy\n",
    "\n",
    "* Gensim\n",
    "\n",
    "You'll explore the prominent features of these libraries and you'll learn how to install them on your computer. But in case you want some other cool packages to play with, you can check out the following resources:\n",
    "\n",
    "* TextBlob\n",
    "\n",
    "* Stanford Core NLP\n",
    "\n",
    "* FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing when to use which package depends on your use case.  NLTK, for example, has a modularized structure that makes it excellent for learning and exploring NLP concepts, but it's not intended for production. It's the most famous Python NLP library, and it's led to important breakthroughs in the field. NLTK is also popular for education and research; it describes itself as an \"an amazing library to play with natural language.\" But NLTK has two major drawbacks: First, learning how to use it effectively can be a difficult and long process. And second, it works slowly, which means that it isn't suitable for most production applications. Because of this, Thinkful recommends NLTK only as an education and research tool.\n",
    "\n",
    "You can install NLTK by running the following command in your command prompt (or Terminal, if you're using a Mac):\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "spaCy is an NLP library that's designed to be easy to use, quick, and production ready. It's a relatively new library, and it isn't as widely used as some other NLP libraries, but its streamlined design and fast performance make it an excellent option. spaCy is minimal and opinionated—and unlike NLTK, it won't overwhelm you with options. Instead, it will select the best algorithm for each purpose—so instead of spending your time choosing between algorithm options, you can focus on being productive. And because it's built on *Cython* (which you'll learn about in the next checkpoint), it's also extremely fast.\n",
    "\n",
    "You can install spaCy by running the following command in your command prompt (or Terminal, if you're using a Mac):\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "Gensim is a specialized library that is optimized for topic modeling and similarity detection. It isn't a general-purpose NLP library, like NLTK and spaCy, but it's invaluable for anyone working on the tasks that it does handle. Its topic modeling algorithms are among the best available, and it's an intuitive, efficient, and scalable tool.\n",
    "\n",
    "You can install Gensim by running the following command in your command prompt (or Terminal, if you're using a Mac):\n",
    "\n",
    "```bash\n",
    "pip install gensim\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
