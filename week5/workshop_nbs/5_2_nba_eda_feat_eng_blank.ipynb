{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data Analysis\n",
    "\n",
    "The data used in this notebook was downloaded from [Kaggle](https://www.kaggle.com/drgilermo/nba-players-stats#Seasons_Stats.csv).  The original source of the data is [Basketball-reference](http://www.basketball-reference.com/).\n",
    "\n",
    "\n",
    "## General Intro EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_url = \"https://docs.google.com/spreadsheets/d/1m0jaYL1KGjxW1cKJUQxVTcPOnm7v7NZEBKRZADCmc68/export?format=csv\"\n",
    "nba = pd.read_csv(data_url, index_col=0)\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = nba.drop(columns=[\"blank2\", \"blanl\"])\n",
    "nba = nba.dropna(subset=[\"Year\", \"Player\", \"Pos\", \"Tm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We have a lot of useful data here, but most predictive models that we'll be looking at only like numeric data.  To still use our information we have to do a little bit of reformatting.\n",
    "\n",
    "### One Hot Encoding / Dummy Encoding\n",
    "\n",
    "For example, for team, we might \"one-hot encode\" (aka create dummy variables).  This is a way of creating a series of variables indicating True/False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe that is a subset of the `nba` dataframe.  Only include in this subset:\n",
    "\n",
    "* Columns: `PTS`, `Player`, & `Tm`\n",
    "* Rows: a random selection of 15 rows (use 42 as the `random_state`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# subset columns\n",
    "nba_sub = ____\n",
    "\n",
    "# subset rows\n",
    "nba_sub = ____\n",
    "\n",
    "nba_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pd.get_dummies()` on the subset.\n",
    "\n",
    "* What happened?\n",
    "* What might we change about this and why?\n",
    "* What does the `drop_first` argument of `pd.get_dummies()` do and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues that come up with using `pd.get_dummies` in a machine learning workflow.  For today, we'll stick with it due to its ease of use compared to more powerful options.\n",
    "\n",
    "Using [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) overcomes the issues that `pd.get_dummies` can run into, but it has a little more complex usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary column named `is_old` that shows whether or not the `Year` variable is before 1980."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary column named `is_california` that shows whether or not a team is located in california."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_teams = [\"LAL\", \"LAC\", \"GSW\", \"SAC\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up some data to be ordinal encoded.\n",
    "\n",
    "* Using the `grades` list create a sample of 20 random letters\n",
    "* Create a 1 column DataFrame from this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "grades = [\"A\", \"B\", \"C\", 'D', 'F']\n",
    "rand_grades = ____\n",
    "\n",
    "grade_df = ____\n",
    "grade_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable that is an ordinal encoding of grade.  Have `A` be 1 and `F` be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Some methods we'll see are sensitive to our variables being on different scales.  For example, if you have variables for a person's height and their annual income, the height feature will have a much much smaller value than the income feature.  In some methods, this will lead to the income variable being a louder signal than the height variable.  Larger magnitude variables can end up drowning out smaller magnitude ones, and this can be an issue if we think height will be an important predictor.\n",
    "\n",
    "To address this issue, we can scale the variables to have equal footing.  This won't change the shape of their distribution.  Not changing shape means that the patterns within and between the variable aren't lost by scaling, the patterns are preserved, the values have just been standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a subset of the nba dataset that has the columns `PTS` and `Age`.\n",
    "* Drop all NAs\n",
    "* Use the pandas boxplot method on this resulting data.\n",
    "* Plot these variables on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to split into groups to evaluate 2 different scalers.  The below code will decide the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data_scientists = [\"Seiedeshiva\", \"Christopher\", \"Jason\", \"Francis\", \n",
    "                   \"Tizeta\", \"Matthew\", \"Jason\", \"Scott\", \"DÃ¦yva\", \n",
    "                   \"Michael\", \"Cristina\", \"Alex\", \"Mike\", \"Taylor\"]\n",
    "# fmt: on\n",
    "\n",
    "# Randomize order\n",
    "np.random.shuffle(data_scientists)\n",
    "\n",
    "n = len(data_scientists) // 2\n",
    "print(f\"Use StandardScaler: {data_scientists[:n]}\")\n",
    "print(f\"Use MinMaxScaler: {data_scientists[n:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick your poison (comment out the one that your group isn't doing)\n",
    "scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use a scaler to scale the `PTS` and `Age` data.\n",
    "* The output of the scaler is a numpy array, convert this back to a dataframe\n",
    "* Recreate the same box plots from before.\n",
    "  * What's the same?\n",
    "  * What's different?\n",
    "  * What's the minimum value of the numeric axis? the max value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# .fit() methods 'learn' something from your data\n",
    "# They don't apply any of these learnings\n",
    "# In the case of a scaler we have to call .transform\n",
    "# Alternatively, we could use .fit_transform() to do \n",
    "# both of these things in one step\n",
    "scaler.fit(____)\n",
    "\n",
    "scaled = scaler.transform(____)\n",
    "\n",
    "scaled_df = ____\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bonus: what attributes does your scaler have? What is the significance of these?\n",
    "* Bonus Bonus: can you recreate this same scaling from scratch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
