{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['dark_background'])\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# K-nearest neighbors classifiers\n",
    "\n",
    "So far, this program has covered learning via linear classifiers (logistic regression) and learning via errors (linear regression). In this checkpoint, you'll look at learning via similarity. This means that you will look for the data points that are most similar to the observation that you are trying to predict.\n",
    "\n",
    "Start with the simplest method: the *nearest neighbor* method.\n",
    "\n",
    "## Nearest neighbor\n",
    "\n",
    "For this example, your challenge is to classify a song as either `rock` or `jazz`. For this data, you have measures of duration in seconds, and measures of loudness in loudness units. (You're not going to be using decibels because that isn't a linear measure, so it would create some problems that you'll get into later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b793791b67431c82bf6d430202fc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "music = pd.DataFrame()\n",
    "\n",
    "# Some data to play with\n",
    "music['duration'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "music['loudness'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "\n",
    "# You know whether the songs in the training data are jazz or not\n",
    "music['jazz'] = [ 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "                  0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "                  1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
    "\n",
    "\n",
    "# Look at the data\n",
    "plt.scatter(\n",
    "    music[music['jazz'] == 1].duration,\n",
    "    music[music['jazz'] == 1].loudness,\n",
    "    color='red'\n",
    ")\n",
    "plt.scatter(\n",
    "    music[music['jazz'] == 0].duration,\n",
    "    music[music['jazz'] == 0].loudness,\n",
    "    color='blue'\n",
    ")\n",
    "plt.legend(['Jazz', 'Rock'])\n",
    "plt.title('Jazz and Rock Characteristics')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Loudness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The simplest form of a similarity model is the nearest neighbor model. This works in a straightforward way: when you're trying to predict an observation, find the closest (or *nearest*) known observation in the training data and use that value to make your prediction. Here you'll use the model as a classifier. The outcome of interest will be a category.\n",
    "\n",
    "To find which observation is \"nearest,\" you need some kind of way to measure distance. Typically, data scientists use *Euclidean distance*, which is the standard distance measure that you may be familiar with from geometry. With two observations—$(x_1, x_2, ...,x_n)$ and $(w_1, w_2,...,w_n)$—in $n$ dimensions, you have the following distance equation:\n",
    "\n",
    "$$ \\sqrt{(x_1-w_1)^2 + (x_2-w_2)^2+...+(x_n-w_n)^2} $$\n",
    "\n",
    "You might recognize this formula (taking distances, squaring them, adding the squares together, and taking the root) as a generalization of the [Pythagorean theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem) into $n$ dimensions. You can technically define any distance measure that you want, and there are times where this customization may be valuable. As a general standard, however, you'll use Euclidean distance.\n",
    "\n",
    "Now that you have a distance measure from each point in the training data to the point that you're trying to predict, the model can find the data point with the smallest distance and then apply that category to your prediction.\n",
    "\n",
    "Try running this model using the scikit-learn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbors = KNeighborsClassifier(n_neighbors=1)\n",
    "X = music[['loudness', 'duration']]\n",
    "Y = music.jazz\n",
    "neighbors.fit(X,Y)\n",
    "\n",
    "## Predict for a song with 24 loudness that's 190 seconds long.\n",
    "neighbors.predict([[24, 190]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "It's as simple as that. It looks like the model is predicting that a 24-loudness, 190-second-long song isn't jazz. All it takes to train the model is a DataFrame of independent variables and a DataFrame of dependent outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You'll note that this example used the `KNeighborsClassifier()` method from scikit-learn. This is because nearest neighbor is a simplification of K-nearest neighbors. The jump, however, isn't that far.\n",
    "\n",
    "## K-nearest neighbors\n",
    "\n",
    "*K-nearest neighbors* (or KNN) is the logical extension of nearest neighbor. Instead of looking at just the single nearest data point to predict an outcome, you look at several of the nearest neighbors, with $k$ representing the number of neighbors that you choose to look at. Each of the $k$ neighbors gets to vote on what the predicted outcome should be.\n",
    "\n",
    "This does a couple of valuable things. Firstly, it smooths out the predictions. If only one neighbor gets to influence the outcome, the model explicitly overfits to the training data. Any single outlier can create pockets of one category prediction surrounded by a sea of the other category.\n",
    "\n",
    "This also means that instead of just predicting classes, you get implicit probabilities. If each of the $k$ neighbors gets a vote on the outcome, then the probability of the test example being from any given class $i$ is as follows:\n",
    "$$ \\frac{votes_i}{k} $$\n",
    "\n",
    "This applies for all classes present in the training set. This example only has two classes, but this model can accommodate as many classes as the dataset necessitates. To come up with a classifier prediction, it simply takes the class for which that fraction is maximized.\n",
    "\n",
    "Now, expand the initial nearest neighbors model from above to a KNN model with a $k$ of `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.4 0.6]]\n"
     ]
    }
   ],
   "source": [
    "neighbors = KNeighborsClassifier(n_neighbors=5)\n",
    "X = music[['loudness', 'duration']]\n",
    "Y = music.jazz\n",
    "neighbors.fit(X,Y)\n",
    "\n",
    "## Predict for a 24-loudness, 190-second-long song.\n",
    "print(neighbors.predict([[24, 190]]))\n",
    "print(neighbors.predict_proba([[24, 190]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now the test prediction has changed. In using the five nearest neighbors, it appears that there were two votes for rock and three for jazz, so it was classified as a jazz song. This is different from the simpler nearest neighbors model. Although the closest observation was in fact rock, there are more jazz songs in the nearest $k$ neighbors than rock.\n",
    "\n",
    "You can visualize your decision bounds with something called a *mesh*, which allows you to generate a prediction over the whole space. Read the code below and make sure that you can identify what the individual lines do. If necessary, consult the documentation for unfamiliar methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# The data. Converting from DataFrames to arrays for the mesh.\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Mesh size\n",
    "h = 4.0\n",
    "\n",
    "# Plot the decision boundary and assign a color to each point in the mesh\n",
    "x_min = X[:, 0].min() - .5\n",
    "x_max = X[:, 0].max() + .5\n",
    "y_min = X[:, 1].min() - .5\n",
    "y_max = X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "# Add the training points to the plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel('Loudness')\n",
    "plt.ylabel('Duration')\n",
    "plt.title('Mesh visualization')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the visualization above, any new point that fell within a blue area would be predicted to be jazz, and any point that fell within a brown area would be predicted to be rock.\n",
    "\n",
    "The boundaries above are strangely jagged here, and you'll get into that in more detail in the next checkpoint.\n",
    "\n",
    "Also note that the visualization isn't completely continuous. There are an infinite number of points in this space, and you can't calculate the value for each one. That's where the mesh comes in. Above, the mesh size was set to 4.0 (`h = 4.0`), which means that you calculate the value for each point in a grid where the points are spaced 4.0 away from each other.\n",
    "\n",
    "You can make the mesh size smaller to get a more continuous visualization, but that comes at the cost of a more computationally demanding calculation. In the cell below, recreate the plot above with a mesh size of 10.0. Then reduce the mesh size until you get a plot that looks good but still renders in a reasonable amount of time. When do you get a visualization that looks acceptably continuous? When do you start to get a noticeable delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play with different mesh sizes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now you've built a KNN model!\n",
    "\n",
    "## Challenge: Implement the nearest neighbor algorithm \n",
    "\n",
    "The nearest neighbor algorithm is extremely simple. So simple, in fact, that you should be able to build it yourself from scratch using the Python that you already know. Code a nearest neighbors algorithm that works for two-dimensional data. You can use either arrays or DataFrames to do this. Then test it against the scikit-learn package on the music dataset from above to ensure that it's correct. The goal here is to confirm your understanding of the model and continue to practice your Python skills. You can simply use a brute force method here. After doing this, look up \"ball tree\" methods to see a more performant algorithm design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "# Some data to play with\n",
    "df['x'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "df['y'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "\n",
    "# You know whether the songs in the training data are jazz or not\n",
    "df['z'] = [ 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
    "                  0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
    "                  1, 1, 1, 1, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your nearest neighbor algorithm here\n",
    "def nearest_neighbor(x, y, df):\n",
    "    df['distance'] = np.sqrt(((x - df['x'])**2) + ((y-df['y'])**2))\n",
    "    pred_val = df.sort_values('distance').head(1)['z']\n",
    "    return np.array(pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor(24, 90, df)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
