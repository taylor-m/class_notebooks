{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So far, this program has covered decision trees growing with an entropy criterion. In doing so, however, the program glossed over how that actually works. In this checkpoint, you'll get more detail into how an algorithm that does that would practically function.\n",
    "\n",
    "Here, you'll learn about one popular algorithm for building a decision tree using entropy and information gain: *ID3*.\n",
    "\n",
    "## The ID3 algorithm\n",
    "\n",
    "ID3 stands for *Iterative Dichotomizer 3*, and it's one of the simplest ways to create a decision tree. ID3 can be generalized into more robust scenarios, but the implementation is based on the framework that you'll go over here. Essentially, ID3 goes through every feature to find the most valuable attribute, and then it splits the node based on that attribute. Then it moves further and further down the tree until it either has a pure class or meets a terminating condition. The details are explored further below.\n",
    "\n",
    "Before you can start working with ID3, however, there are some requirements for the data in this most basic form. Firstly, outcomes have to be binary. The simplest version of ID3 is a binary classifier. Also, the attributes that you use to build the tree have to be categorical. Attributes can have many categories, but they must be known and countable.\n",
    "\n",
    "If those two criteria are met, then you can build a basic ID3 classifying algorithm.\n",
    "\n",
    "The other thing that you'll need for this is the definition of entropy. Recall from the previous assignment that you're going to use Shannon Entropy $H$, defined as follows:\n",
    "\n",
    "$$ H = -\\sum_{i=1}^n P(x_i)log_2 P(x_i) $$\n",
    "\n",
    "For simplicity of calculation, you're actually going to do a slight transform on this definition. Recall from a (quite possibly long ago) algebra class that you can bring exponentials out to the front of a logarithm. In this case, you'll raise the probability to `-1`, changing the formula to the following:\n",
    "\n",
    "$$ H = \\sum_{i=1}^n P(x_i)log_2 \\frac{1}{P(x_i)} $$\n",
    "\n",
    "This removes the negative sign up front, and will make it easier for you to implement this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating entropy\n",
    "\n",
    "Because this algorithm is based on entropy, go over a quick example of how to calculate it.\n",
    "\n",
    "Say that you have 20 students, and you're trying to classify them as male or female. The only attribute that you have is whether their height is tall, medium, or short. Of the 20 students, 12 are boys and 8 are girls. Of the 12 boys, 4 are tall, 6 are medium, and 2 are short. Of the 8 girls, 1 is tall, 2 are medium, and 5 are short.\n",
    "\n",
    "What is the entropy before any rule is applied? And what is the entropy after applying a rule for being tall?\n",
    "\n",
    "The initial entropy is just the formula plugged in over both the possible classes of interest:\n",
    "\n",
    "$$ H = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "\n",
    "$$ = \\frac{12}{20}*log_2 \\frac{20}{12} + \\frac{8}{20}*log_2 \\frac{20}{8} = .971 $$\n",
    "\n",
    "What if you apply the rule `height = short`? You need to calculate the weighted average of the two entropies, one for the short students and one for the students who aren't short.\n",
    "\n",
    "$$ H(short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{2}{7}*log_2 \\frac{7}{2} + \\frac{5}{7}*log_2 \\frac{7}{5} = .863 $$\n",
    "\n",
    "$$ H(not\\_short) = P(male)*log_2 \\frac{1}{P(male)} + P(female)*log_2 \\frac{1}{P(female)}  $$\n",
    "\n",
    "$$ = \\frac{10}{13}*log_2 \\frac{13}{10} + \\frac{3}{13}*log_2 \\frac{13}{3} = .779 $$\n",
    "\n",
    "Note that all the probabilities here are conditional on the criteria that you're assuming (short or not short). The weighted average of the two is this:\n",
    "\n",
    "$$ P(short) * H(short) + P(not\\_short) * H(not\\_short) = \\frac{7}{20} * .863 + \\frac{13}{20} * .779 = .809 $$\n",
    "\n",
    "So the entropy from this question would go from `0.971` to `0.809`. That's an improvement! Use the space below to calculate the entropy of the other criteria, because you also know whether the students are tall or medium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280757477080679\n",
      "0.9245112497836531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Put your calculations below\n",
    "\n",
    "H_tall = ((4/5)*log2((4/5)**-1)) + ((1/5)*log2((1/5)**-1))\n",
    "\n",
    "H_not_tall = ((8/15)*log2((8/15)**-1)) + ((7/15)*log2((7/15)**-1))\n",
    "\n",
    "H_medium = ((6/8)*log2((6/8)**-1)) + ((2/8)*log2((2/8)**-1))\n",
    "\n",
    "H_not_medium = ((6/12)*log2((6/12)**-1)) + ((6/12)*log2((6/12)**-1))\n",
    "\n",
    "\n",
    "entropy_tall = (H_tall * (5/20)) + (H_not_tall * (15/20))\n",
    "\n",
    "entropy_med = (H_medium * (8/20)) + (H_not_medium * (12/20))\n",
    "\n",
    "print(entropy_tall)\n",
    "print(entropy_med)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The example solution is below here. Don't peek!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280757477080679 0.9245112497836532\n"
     ]
    }
   ],
   "source": [
    "# Example solution\n",
    "\n",
    "# Tall\n",
    "H_tall = 4 / 5 * log2(5 / 4) + 1 / 5 * log2(5 / 1)\n",
    "H_not_tall = 8 / 15 * log2(15 / 8) + 7 / 15 * log2(15 / 7)\n",
    "\n",
    "entropy_tall = 5 / 20 * H_tall + 15 / 20 * H_not_tall\n",
    "\n",
    "\n",
    "# Medium\n",
    "H_medium = 6/8 * log2(8/6) + 2/8 * log2(8/2)\n",
    "H_not_medium = 6/12 * log2(12/6) + 6/12 * log2(12/6)\n",
    "\n",
    "entropy_medium = 8/20 * (H_medium) + 12/20 * (H_not_medium)\n",
    "\n",
    "print(entropy_tall, entropy_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You should have found entropies of `0.925` for medium and `0.928` for tall. Both of those entropies are higher. As you know, you want to prioritize the questions with the most information gain. Which one of these questions would that be?\n",
    "\n",
    "Asking if an individual is short provides the most information gain. Also, note that for all possible questions, you're still comparing with the same initial entropy value. So one way of seeing which question has the most information gain is to find the one with the lowest entropy. This makes sense when you think of entropy as uncertainty; the less uncertainty after a question, the more information you gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocoding the algorithm\n",
    "\n",
    "*Pseudocode* is the process of writing the steps and logic that you would implement in code, but in normal language rather than in commands that a programming language could execute. It can be a useful way to chart out how you want to build an algorithm, and it's a common topic for technical interviews. Here you'll use pseudocode to explore the ID3 algorithm.\n",
    "\n",
    "Here is some reasonable pseudocode for ID3. This pseudocode will then be followed up with an explanation of the steps. The outcome for this variable will be `A` or `B`. An attribute is denoted as a<sub>i</sub>, and a value of that attribute is v<sub>i</sub>.\n",
    "\n",
    "\n",
    "<pre>\n",
    "Algorithm(Observations, Outcome, Attributes)\n",
    "    Create a root node.\n",
    "    If all observations are 'A', label root node 'A' and return.\n",
    "    If all observations are 'B', label root node 'B' and return.\n",
    "    If no attributes return the root note labeled with the most common Outcome.\n",
    "    Otherwise, start:\n",
    "        For each value v<sub>i</sub> of each attribute a<sub>i</sub>, calculate the entropy.\n",
    "        The attribute a<sub>i</sub> and value v<sub>i</sub> with the lowest entropy is the best rule.\n",
    "        The attribute for this node is then a<sub>i</sub>\n",
    "            Split the tree to below based on the rule a<sub>i</sub> = v<sub>i</sub>\n",
    "            Observations<sub>v<sub>i</sub></sub> is the subset of observations with value v<sub>i</sub>\n",
    "            If Observations<sub>v<sub>i</sub></sub> is empty cap with node labeled with most common Outcome\n",
    "            Else at the new node start a subtree (Observations<sub>v<sub>i</sub></sub>, Target Outcome, Attributes - {a<sub>i</sub>}) and repeat the algorithm\n",
    "</pre>\n",
    "\n",
    "Now, walk through this pseudocode algorithm in plain English, piece by piece.\n",
    "\n",
    "First, you create a root node. Simple enoughâ€”you have to start with something.\n",
    "\n",
    "The next two lines say that if you're already exclusively one class, just label with that class and you're done. Similarly, the following line says that if you don't have any attributes left, you're also done, labeling with the most common outcome.\n",
    "\n",
    "Then you get into the real algorithm. First, you have to find the best attribute by calculating entropies for all possible values. The attribute-value pair with the lowest entropy is then the best attribute. That's the attribute that you give to the node.\n",
    "\n",
    "You use that rule to split the node, creating subsets of observations. There are then two new nodes, each with a subset of the observations corresponding to the rule. If observations are null, then label with the dominant outcome.\n",
    "\n",
    "Otherwise, at each new node, start the algorithm again.\n",
    "\n",
    "This is how a decision tree actually functions. Understanding this will give you some insight into how this algorithm works and reveals several attributes of the algorithm. Firstly, the solution is not necessarily optimal. The tree can get stuck in local optima, just like with the gradient descent algorithm. It also has no way to work backward if it finds itself in an informationless space. You can add criteria to make it stop before the tree has grown to run out of attributes or all leaf nodes are single classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill\n",
    "\n",
    "Look over the code for [this real ID3 algorithm in Python](https://github.com/NinjaSteph/DecisionTree/blob/master/sna2111_DecisionTree/DecisionTree.py). Note how well the author breaks up functionality into individual, reusable, and well-named helper functions. See if you can match the pseudocode steps to the code in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#find item in a list\n",
    "def find(item, list):\n",
    "    for i in list:\n",
    "        if item(i): \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#find most common value for an attribute\n",
    "def majority(attributes, data, target):\n",
    "    #find target attribute\n",
    "    valFreq = {}\n",
    "    #find target in data\n",
    "    index = attributes.index(target)\n",
    "    #calculate frequency of values in target attr\n",
    "    for tuple in data:\n",
    "        if (valFreq.has_key(tuple[index])):\n",
    "            valFreq[tuple[index]] += 1 \n",
    "        else:\n",
    "            valFreq[tuple[index]] = 1\n",
    "    max = 0\n",
    "    major = \"\"\n",
    "    for key in valFreq.keys():\n",
    "        if valFreq[key]>max:\n",
    "            max = valFreq[key]\n",
    "            major = key\n",
    "    return major\n",
    "\n",
    "#Calculates the entropy of the given data set for the target attr\n",
    "def entropy(attributes, data, targetAttr):\n",
    "\n",
    "    valFreq = {}\n",
    "    dataEntropy = 0.0\n",
    "    \n",
    "    #find index of the target attribute\n",
    "    i = 0\n",
    "    for entry in attributes:\n",
    "        if (targetAttr == entry):\n",
    "            break\n",
    "        ++i\n",
    "    \n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for entry in data:\n",
    "        if (valFreq.has_key(entry[i])):\n",
    "            valFreq[entry[i]] += 1.0\n",
    "        else:\n",
    "            valFreq[entry[i]]  = 1.0\n",
    "\n",
    "    # Calculate the entropy of the data for the target attr\n",
    "    for freq in valFreq.values():\n",
    "        dataEntropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "        \n",
    "    return dataEntropy\n",
    "\n",
    "def gain(attributes, data, attr, targetAttr):\n",
    "    \"\"\"\n",
    "    Calculates the information gain (reduction in entropy) that would\n",
    "    result by splitting the data on the chosen attribute (attr).\n",
    "    \"\"\"\n",
    "    valFreq = {}\n",
    "    subsetEntropy = 0.0\n",
    "    \n",
    "    #find index of the attribute\n",
    "    i = attributes.index(attr)\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for entry in data:\n",
    "        if (valFreq.has_key(entry[i])):\n",
    "            valFreq[entry[i]] += 1.0\n",
    "        else:\n",
    "            valFreq[entry[i]]  = 1.0\n",
    "    # Calculate the sum of the entropy for each subset of records weighted\n",
    "    # by their probability of occuring in the training set.\n",
    "    for val in valFreq.keys():\n",
    "        valProb        = valFreq[val] / sum(valFreq.values())\n",
    "        dataSubset     = [entry for entry in data if entry[i] == val]\n",
    "        subsetEntropy += valProb * entropy(attributes, dataSubset, targetAttr)\n",
    "\n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
    "    # whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(attributes, data, targetAttr) - subsetEntropy)\n",
    "\n",
    "#choose best attibute \n",
    "def chooseAttr(data, attributes, target):\n",
    "    best = attributes[0]\n",
    "    maxGain = 0;\n",
    "    for attr in attributes:\n",
    "        newGain = gain(attributes, data, attr, target) \n",
    "        if newGain>maxGain:\n",
    "            maxGain = newGain\n",
    "            best = attr\n",
    "    return best\n",
    "\n",
    "#get values in the column of the given attribute \n",
    "def getValues(data, attributes, attr):\n",
    "    index = attributes.index(attr)\n",
    "    values = []\n",
    "    for entry in data:\n",
    "        if entry[index] not in values:\n",
    "            values.append(entry[index])\n",
    "    return values\n",
    "\n",
    "def getExamples(data, attributes, best, val):\n",
    "    examples = [[]]\n",
    "    index = attributes.index(best)\n",
    "    for entry in data:\n",
    "        #find entries with the give value\n",
    "        if (entry[index] == val):\n",
    "            newEntry = []\n",
    "            #add value if it is not in best column\n",
    "            for i in range(0,len(entry)):\n",
    "                if(i != index):\n",
    "                    newEntry.append(entry[i])\n",
    "            examples.append(newEntry)\n",
    "    examples.remove([])\n",
    "    return examples\n",
    "\n",
    "def makeTree(data, attributes, target, recursion):\n",
    "    recursion += 1\n",
    "    #Returns a new decision tree based on the examples given.\n",
    "    data = data[:]\n",
    "    vals = [record[attributes.index(target)] for record in data]\n",
    "    default = majority(attributes, data, target)\n",
    "\n",
    "    # If the dataset is empty or the attributes list is empty, return the\n",
    "    # default value. When checking the attributes list for emptiness, we\n",
    "    # need to subtract 1 to account for the target attribute.\n",
    "    if not data or (len(attributes) - 1) <= 0:\n",
    "        return default\n",
    "    # If all the records in the dataset have the same classification,\n",
    "    # return that classification.\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute to best classify our data\n",
    "        best = chooseAttr(data, attributes, target)\n",
    "        # Create a new decision tree/node with the best attribute and an empty\n",
    "        # dictionary object--we'll fill that up next.\n",
    "        tree = {best:{}}\n",
    "    \n",
    "        # Create a new decision tree/sub-node for each of the values in the\n",
    "        # best attribute field\n",
    "        for val in getValues(data, attributes, best):\n",
    "            # Create a subtree for the current value under the \"best\" field\n",
    "            examples = getExamples(data, attributes, best, val)\n",
    "            newAttr = attributes[:]\n",
    "            newAttr.remove(best)\n",
    "            subtree = makeTree(examples, newAttr, target, recursion)\n",
    "    \n",
    "            # Add the new subtree to the empty dictionary object in our new\n",
    "            # tree/node we just created.\n",
    "            tree[best][val] = subtree\n",
    "    \n",
    "    return tree\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
