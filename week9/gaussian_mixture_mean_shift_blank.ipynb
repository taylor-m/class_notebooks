{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqhx94S6rxlf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_guess(\n",
    "    data,\n",
    "    x1_mean,\n",
    "    x1_std,\n",
    "    x2_mean,\n",
    "    x2_std,\n",
    "    n=int(1e5),\n",
    "    prob_1=None,\n",
    "    prob_2=None,\n",
    "    percent_1=0.5,\n",
    "):\n",
    "    \"\"\"Helper function for plotting GMM process\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data:      1d array of data values\n",
    "    x1_mean:   the estimated mean of sample 1\n",
    "    x1_std:    the estimated standard deviation of\n",
    "               sample 1\n",
    "    x2_mean:   the estimated mean of sample 2\n",
    "    x2_std:    the estimated standard deviation of\n",
    "               sample 2\n",
    "    n:         the number of samples to generate for x1 &\n",
    "               x2 plotting samples\n",
    "    prob_1:    1d array of the probability associated with each\n",
    "               observation belonging to group 1\n",
    "    prob_2:    1d array of the probability associated with each\n",
    "               observation belonging to group 2\n",
    "    percent_1: what percentage of data is expected\n",
    "               to be in group 1 (used to control height\n",
    "               of distribution)\n",
    "    \"\"\"\n",
    "    n1 = int(n * percent_1)\n",
    "    n2 = int(n * (1 - percent_1))\n",
    "    sample_1 = np.random.normal(x1_mean, x1_std, n1)\n",
    "    sample_2 = np.random.normal(x2_mean, x2_std, n2)\n",
    "\n",
    "    sns.kdeplot(sample_1, label=\"Group 1\")\n",
    "    sns.kdeplot(sample_2, label=\"Group 2\")\n",
    "\n",
    "    if prob_1 is not None and prob_2 is not None:\n",
    "        prob_1 = np.array(prob_1).reshape(-1, 1)\n",
    "        prob_2 = np.array(prob_2).reshape(-1, 1)\n",
    "\n",
    "        prob_1 = MinMaxScaler().fit_transform(prob_1)\n",
    "        prob_2 = MinMaxScaler().fit_transform(prob_2)\n",
    "\n",
    "        for x, p1, p2 in zip(data, prob_1, prob_2):\n",
    "            if p1 > p2:\n",
    "                c = \"blue\"\n",
    "                alpha = p1[0]\n",
    "            else:\n",
    "                c = \"orange\"\n",
    "                alpha = p2[0]\n",
    "\n",
    "            plt.scatter(x, 0, alpha=alpha * 0.8, c=c)\n",
    "    else:\n",
    "        plt.scatter(data, [0 for _ in data], c=\"black\")\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min = 35.71\n",
    "lat_max = 36.55\n",
    "lon_min = -84.55\n",
    "lon_max = -82.37\n",
    "\n",
    "# Potential extra practice:\n",
    "#  * plot all the starbs locations in one color\n",
    "#  * plot my location in a different color\n",
    "adam_lat = 36.3\n",
    "adam_lon = -82.4\n",
    "\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/starbucks_locations.csv\"\n",
    "starbs = pd.read_csv(data_url)\n",
    "\n",
    "starbs = starbs[starbs[\"Latitude\"].between(lat_min, lat_max)]\n",
    "starbs = starbs[starbs[\"Longitude\"].between(lon_min, lon_max)]\n",
    "starbs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Warm up üî•\n",
    "\n",
    "According to the distribution of our filtered data in the `starbs` dataframe.\n",
    "\n",
    "We're most likely to find a starbucks at which of these values of `'Longitude'`?\n",
    "* (A) -81\n",
    "* (B) -83\n",
    "* (C) -84\n",
    "* (D) -85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the distribution of our filtered data in the `starbs` dataframe.\n",
    "\n",
    "We're most likely to find a starbucks at which of these values of `'Latitude'`?\n",
    "* (A) 35\n",
    "* (B) 36\n",
    "* (C) 37\n",
    "* (D) 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzkRHuE9z25r"
   },
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "gaussian distribution = normal distribution\n",
    " \n",
    "### MLE\n",
    "\n",
    "We're given the below data, and we're told that the data is measurements of some snail characteristic üêå.  We want to try and figure out what the *population* distribution looks like.  Remember we just have a *sample*, but the population is all of the snail species we're studying.  Having a good estimate of this population distribution can benefit and guide our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = [ 8.521, 16.586, 11.154,  3.323, 13.662, 14.649,  6.149,  5.528,\n",
    "        18.871, 11.498,  8.921,  5.776,  7.292,  6.638, 13.321,  7.073,\n",
    "         8.827, 10.375,  1.645, 13.566, 19.846,  6.347,  8.617, 14.462,\n",
    "         4.483, 11.170, 11.322,  5.710, 11.311,  7.672,  9.765, 14.443,\n",
    "        18.360,  9.304, 10.247, 10.955, 14.194,  8.344,  5.783, 12.533,\n",
    "        12.937,  0.846,  4.925,  9.006, 11.443, 16.160, 10.751,  8.513,\n",
    "        23.865, 17.228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before plotting:\n",
    "    * What kind of distribution do you expect this type of data to follow?\n",
    "    * What 'parameters' does that distribution have?\n",
    "        * i.e. these are what you'd need to give `np.random.<distribution_name>()` so it can know what shape of distribution you want random numbers from\n",
    "\n",
    "\n",
    "* Now let's plot a histogram.  Does the shape of the data's distribution support your hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we're after the population distribution.\n",
    "\n",
    "* Re-plot the histogram\n",
    "* Add a vertical line at 20 (color it and give it a label for the legend)\n",
    "* Add a vertical line at 10 (color it and give it a label for the legend)\n",
    "* Add a vertical line at  5 (color it and give it a label for the legend)\n",
    "\n",
    "Let's say these are 3 guesses at what the population mean are.  Given our data, which of these is the most *likely*.  Due to the nature of random sampling, it's possible that all 3 of these are valid, but it's not practical to assume that we got a very unusual random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're engaging in a process called Maximum Likelihood Estimation (MLE).  We're trying to Estimate the population mean based on what's most *likely*.  We want our Estimate to have the Maximum Likelihood of being correct.\n",
    "\n",
    "Below is a visualization of us trying to find the population mean via MLE.\n",
    "\n",
    "Note, this is why the mean parameter of `np.random.normal` is called `loc`.\n",
    "\n",
    "<img src='images/mean_mle.gif' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more parameter we'd need to esimate in order to fully describe our distribution's shape.  This is why this parameter in `np.random.normal` is called `scale` (we scale the width, the height is derived from the width).\n",
    "\n",
    "<img src='images/std_mle.gif' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the maximum likelihood estimates for the population mean and standard deviation are the equal to the sample mean and sample standard deviations.\n",
    "\n",
    "* Generate 1000 random data points from our estimate popuation distribution\n",
    "* Plot this resulting distribution, compare it to our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing\n",
    "\n",
    "We just got some new data.  This data was collected from 2 separate species of snails üêå.\n",
    "\n",
    "We know these snails each follow a normal distribution, and we know that they have different means & standard deviations.  Unfortunately, the scientist in the field didn't write down which species each observation is, so we have to try and figure out these 2 separate distributions from a sample of mixed data.\n",
    "\n",
    "If we knew the species labels, we could perform the same MLE process we did above (filter and perform MLE 1 at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = [11.83984961, 12.37143473, 26.15417807, 28.58500880, 27.70571253,\n",
    "        24.24028217, 18.33611103, 15.28117383, 14.57235710, 18.49006327,\n",
    "        37.83761751, 18.82148403, 36.62430095, 26.61444903, 15.3433858 ,\n",
    "        24.60865873, 31.67437436, 26.08487739, 14.75279305, 25.63485726,\n",
    "        30.44683604, 29.64163292, 14.91536797, 20.48912193, 27.97187397,\n",
    "        11.41235662, 17.90399557, 33.82514212, 17.71352474, 25.98954934,\n",
    "        19.86878159, 26.92304096, 16.25738730, 29.10667734, 31.06548273,\n",
    "        21.14768063, 29.93913722, 25.32381510, 18.98788655, 16.60772929,\n",
    "        25.00896332, 17.41901911, 14.21902871, 27.90108363, 26.99118323,\n",
    "        26.03784060, 31.83483958, 25.73633429, 31.48278996, 24.23683382,\n",
    "        19.24019041, 14.73365444, 27.70687662, 19.82397780, 14.58054905,\n",
    "        22.51116415, 21.31616800, 26.34025573, 28.45094146, 24.61646750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the data.  Do you think we'll be able to separate out 2 normal distributions from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure this out, we'll throw in some initial guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_x1_mean = ____\n",
    "guess_x1_std = ____\n",
    "guess_x2_mean = ____\n",
    "guess_x2_std = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=guess_x1_mean,\n",
    "    x1_std=guess_x1_std,\n",
    "    x2_mean=guess_x2_mean,\n",
    "    x2_std=guess_x2_std,\n",
    ")\n",
    "plt.title(\"initial guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to these distributions, we can now assign a probability (or likelihood) that each point came from each distribution.  Using this, we can split the data into 2 groups:\n",
    "\n",
    "* (1) points more likely to have come from the orange distribution\n",
    "* (2) points more likely to have come from the blue distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"mixture\"])\n",
    "df[\"prob_blue\"] = stats.norm(guess_x1_mean, guess_x1_std).pdf(df[\"mixture\"])\n",
    "df[\"prob_orange\"] = stats.norm(guess_x2_mean, guess_x2_std).pdf(df[\"mixture\"])\n",
    "df[\"label\"] = \"blue\"\n",
    "df.loc[df[\"prob_orange\"] > df[\"prob_blue\"], \"label\"] = \"orange\"\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=guess_x1_mean,\n",
    "    x1_std=guess_x1_std,\n",
    "    x2_mean=guess_x2_mean,\n",
    "    x2_std=guess_x2_std,\n",
    "    prob_1=df[\"prob_blue\"],\n",
    "    prob_2=df[\"prob_orange\"],\n",
    ")\n",
    "plt.title(\"initial guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2 individual groups of data! And just like in the single species example, we can now make a better guess about what each distribution looks like.  We can use this to update our guess of the distribution shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives = df.groupby(\"label\").agg({\"mixture\": [\"mean\", \"std\"]})\n",
    "descriptives.columns = [\"mean\", \"std\"]\n",
    "descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_x1_mean = descriptives.loc[\"blue\", \"mean\"]\n",
    "guess_x1_std = descriptives.loc[\"blue\", \"std\"]\n",
    "guess_x2_mean = descriptives.loc[\"orange\", \"mean\"]\n",
    "guess_x2_std = descriptives.loc[\"orange\", \"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=guess_x1_mean,\n",
    "    x1_std=guess_x1_std,\n",
    "    x2_mean=guess_x2_mean,\n",
    "    x2_std=guess_x2_std,\n",
    "    prob_1=df[\"prob_blue\"],\n",
    "    prob_2=df[\"prob_orange\"],\n",
    ")\n",
    "plt.title(\"updated guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And repeat! This process we just went through goes by the name Gussian Mixture Modeling.  Gaussian is another name for the normal distribution (named after a dude who contributed a lot to math and statistics).\n",
    "\n",
    "Let's see how we can approach this problem with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"mixture\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(2)\n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the means and standard deviations that sklearn settled on after going through that process a couple more iterations.  Note that in this simple case, our 1 iteration algorithm got pretty similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_mean, x2_mean = gmm.means_\n",
    "x1_std, x2_std = np.sqrt(gmm.covariances_)\n",
    "\n",
    "x1_mean = x1_mean[0]\n",
    "x2_mean = x2_mean[0]\n",
    "x1_std = x1_std[0][0]\n",
    "x2_std = x2_std[0][0]\n",
    "\n",
    "print(f\"Cluster 1 - mean: {x1_mean:.2f}; std: {x1_std:.2f}\")\n",
    "print(f\"Cluster 2 - mean: {x2_mean:.2f}; std: {x2_std:.2f}\")\n",
    "\n",
    "group_probs = gmm.predict_proba(X)\n",
    "prob_1 = group_probs[:, 0]\n",
    "prob_2 = group_probs[:, 1]\n",
    "\n",
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=x1_mean,\n",
    "    x1_std=x1_std,\n",
    "    x2_mean=x2_mean,\n",
    "    x2_std=x2_std,\n",
    "    prob_1=prob_1,\n",
    "    prob_2=prob_2,\n",
    ")\n",
    "plt.title(\"sklearn's guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool and all, but our data almost never has just a single column (i.e. this example was *univariate*).  Let's look at a case with multiple variables (i.e. *multivariate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon = starbs[[\"Latitude\", \"Longitude\"]].copy()\n",
    "\n",
    "fig = px.scatter_geo(lat_lon, \"Latitude\", \"Longitude\", scope=\"usa\")\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try and visualize both distributions at once using various different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    starbs, \"Longitude\", \"Latitude\", marginal_x=\"histogram\", marginal_y=\"histogram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = starbs[\"Longitude\"]\n",
    "y = starbs[\"Latitude\"]\n",
    "kernel = stats.gaussian_kde((x, y))\n",
    "\n",
    "x = np.linspace(x.min(), x.max(), 100)\n",
    "y = np.linspace(y.min(), y.max(), 100)\n",
    "\n",
    "z = []\n",
    "for xi in x:\n",
    "    zi = []\n",
    "    for yi in y:\n",
    "        zi.append(kernel((xi, yi))[0])\n",
    "\n",
    "    z.append(zi)\n",
    "\n",
    "z = np.array(z)\n",
    "z.reshape((x.shape[0], y.shape[0]))\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like KMeans, we need to decide up front how many clusters we want the clustering process to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAu-thrM0ICo"
   },
   "outputs": [],
   "source": [
    "k = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create and fit a `GaussianMixture()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare `GaussianMixture()` to `KMeans()`.\n",
    "\n",
    "* Create and fit a `KMeans()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the resulting centroids from each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `GaussianMixture.predict_proba()` method.\n",
    "* What are the top 3 observations we are least confident about?\n",
    "* What are the top 3 observations we are most confident about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3521,
     "status": "ok",
     "timestamp": 1580406740855,
     "user": {
      "displayName": "Adam Spannbauer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU2JSQZkFVVbFv_OhPGdmiwr3ChGbq34PCZXJz=s64",
      "userId": "04097551985177324740"
     },
     "user_tz": 300
    },
    "id": "bvauiLwZ0m3v",
    "outputId": "198c2f33-e1af-4aae-a43c-cc4f9bb75ae0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_df = lat_lon.copy()\n",
    "plot_df[\"label\"] = gauss.predict(lat_lon)\n",
    "plot_df = plot_df.sort_values(\"label\")\n",
    "\n",
    "centers_df = pd.DataFrame(gauss.means_, columns=[\"Latitude\", \"Longitude\"])\n",
    "centers_df[\"label\"] = \"Cluster center\"\n",
    "\n",
    "plot_df = pd.concat((plot_df, centers_df), sort=False)\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    plot_df,\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    color=\"label\",\n",
    "    scope=\"usa\",\n",
    "    title=\"Gaussian Mixture Model Results\",\n",
    "    hover_name=plot_df.index,\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "plot_df = lat_lon.copy()\n",
    "plot_df[\"label\"] = kmeans.labels_\n",
    "plot_df = plot_df.sort_values(\"label\")\n",
    "\n",
    "centers_df = pd.DataFrame(kmeans.cluster_centers_, columns=[\"Latitude\", \"Longitude\"])\n",
    "centers_df[\"label\"] = \"Cluster center\"\n",
    "\n",
    "plot_df = pd.concat((plot_df, centers_df), sort=False)\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    plot_df,\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    color=\"label\",\n",
    "    scope=\"usa\",\n",
    "    title=\"KMeans Results\",\n",
    "    hover_name=plot_df.index,\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HVwJByXZztt_"
   },
   "source": [
    "## Mean-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `matplotlib` to plot the East TN Starbucks\n",
    "* Label the `x` and `y` axes\n",
    "* Give the plot a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1662,
     "status": "ok",
     "timestamp": 1580406766461,
     "user": {
      "displayName": "Adam Spannbauer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU2JSQZkFVVbFv_OhPGdmiwr3ChGbq34PCZXJz=s64",
      "userId": "04097551985177324740"
     },
     "user_tz": 300
    },
    "id": "EnPvZhaqih5R",
    "outputId": "26bdc4a0-d4d3-474d-84ce-b2b49cabad23"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `sns.kdeplot()` to show the distribution of Latitude and Longitude separately.\n",
    "* How many clusters do you expect to find if clustering only on one of these variables?\n",
    "* Play with the `bw_method` parameter of `sns.kdeplot()`.  How does this change how many clusters you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `sns.kdeplot()` to show the 2d distribution of Latitude and Longitude.\n",
    "* Add the starbucks locations to the plot.\n",
    "* How many clusters do you expect to find if clustering only on one of these variables?\n",
    "* Play with the `bw_method` parameter of `sns.kdeplot()`.  How does this change how many clusters you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3482,
     "status": "ok",
     "timestamp": 1580408343569,
     "user": {
      "displayName": "Adam Spannbauer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU2JSQZkFVVbFv_OhPGdmiwr3ChGbq34PCZXJz=s64",
      "userId": "04097551985177324740"
     },
     "user_tz": 300
    },
    "id": "IAAWE0zNybug",
    "outputId": "ca5ccd31-de13-4d9c-fd2f-74cf59c04bb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly different view of the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.5\n",
    "\n",
    "x = starbs[\"Longitude\"]\n",
    "y = starbs[\"Latitude\"]\n",
    "kernel = stats.gaussian_kde((x, y), bw_method=bw)\n",
    "\n",
    "x = np.linspace(x.min(), x.max(), 100)\n",
    "y = np.linspace(y.min(), y.max(), 100)\n",
    "\n",
    "z = []\n",
    "for xi in x:\n",
    "    zi = []\n",
    "    for yi in y:\n",
    "        zi.append(kernel((xi, yi))[0])\n",
    "\n",
    "    z.append(zi)\n",
    "\n",
    "z = np.array(z)\n",
    "z.reshape((x.shape[0], y.shape[0]))\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `MeanShift()` to cluster the observations\n",
    "* The default `bandwidth` used by `MeanShift()` is calculated using `sklearn.cluster.estimate_bandwidth()` (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bw = estimate_bandwidth(lat_lon)\n",
    "print(f\"Default bandwidth: {default_bw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1580409516160,
     "user": {
      "displayName": "Adam Spannbauer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU2JSQZkFVVbFv_OhPGdmiwr3ChGbq34PCZXJz=s64",
      "userId": "04097551985177324740"
     },
     "user_tz": 300
    },
    "id": "BLxIF9rXq_l-",
    "outputId": "cfee4236-01d0-45ec-f6d3-7e828a96237f"
   },
   "outputs": [],
   "source": [
    "# Define a variable to hold the selected bw for use in plotting later\n",
    "bw = ___\n",
    "\n",
    "clst = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the resulting cluster centers.  How many clusters were found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Redo the above plot colored by cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra practice in case we have time:\n",
    "\n",
    "* The nba dataset is loaded & cleaned for you below.\n",
    "* Apply mean shift and interpret the clusters.\n",
    "* Based on your interpretation, are these good clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/nba_player_seasons.csv\"\n",
    "nba = pd.read_csv(data_url)\n",
    "\n",
    "nba = nba[(nba[\"GS\"] >= 20) & (nba[\"MP\"] >= 10)]\n",
    "nba = nba.dropna().reset_index(drop=True)\n",
    "nba_og = nba.copy()\n",
    "\n",
    "nba = nba[[\"PTS\", \"TRB\", \"TOV\", \"AST\", \"BLK\", \"Age\"]]\n",
    "nba.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpaRDTSwWwvP5Lgs+zXRY7",
   "collapsed_sections": [],
   "name": "starbucks_gaussian_mixture_mean_shift.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
