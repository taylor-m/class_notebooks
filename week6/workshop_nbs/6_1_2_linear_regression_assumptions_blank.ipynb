{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions and Snails\n",
    "\n",
    "This dataset is about abalone snails.  Resources about the data can be found [here](https://archive.ics.uci.edu/ml/datasets/Abalone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cols = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole_Weight\",\n",
    "    \"Shucked_Weight\",\n",
    "    \"Visecra_Weight\",\n",
    "    \"Shell_Weight\",\n",
    "    \"Rings\",\n",
    "]\n",
    "abalone = pd.read_csv(\n",
    "    \"https://docs.google.com/spreadsheets/d/1GwCnxFT4Sd6iZDj07kNNhEREr7OJQnGvtxd67b5AMio/export?format=csv\",\n",
    "    names=cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to know the data.  Our target variable is `'Rings'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity\n",
    "\n",
    "If we want to model our data with a linear model, we're assuming that there's a linear relationship.\n",
    "\n",
    "We can sort of play around with this one by doing transformations to our inputs.  For example we can easily model $y = x^2$ with a linear model by squaring $x$ before inputting it into our model and making the relationship linear.  Squaring is the goto example for this type of transformation to linearity, but remember some other transforms we've looked at like square root and log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (un)comment different transformations to learn the\n",
    "# shape that these type relationships have in scatterplots\n",
    "# fmt: off\n",
    "def transform(x):\n",
    "#     return np.sqrt(x)\n",
    "#     return np.log(x)\n",
    "    return x**2\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "X = np.arange(1, 30)\n",
    "y = transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].scatter(X, y)\n",
    "axes[1].scatter(transform(X), y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into our abalone snails dataset what linear relationships do we see?  Which ones do you think we could make linear with a transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error has a mean of 0\n",
    "\n",
    "Nothing to add to what's in [the reading](https://courses.thinkful.com/dsbc-regression-problems-v1/checkpoint/3).  Just know that this is referring the theoritical error term that you see written out in formulas.  In practice our mistakes are more formally referred to as 'residuals', these residuals are essentially guaranteed to have a mean of 0 due to how the algorithms (like OLS) fit the model.\n",
    "\n",
    "## Homoscedasticity\n",
    "\n",
    "Big fancy hard to read word that means \"same variance\".  In short, we don't want our errors to have any pattern to them.  The whole reason we're modeling is to capture as many patterns and use these patterns to make accurate predictions.  If we have a pattern to our errors, we should find a way to include this pattern in the model.\n",
    "\n",
    "Below we'll look at an example with `'Length'` and `'Rings'`.  Looking at the scatter plot between the 2 variables we see:\n",
    "  1. They're positively correlated\n",
    "      * The longer the snail the more rings; makes sense\n",
    "  2. The longer the snail the more variation in ring count there is\n",
    "      * This also makes some sense.  Short snails only have so much room to make rings, so they have a smaller range of outcomes; longer snails have more potential for diversity of ring count.\n",
    "      \n",
    "Number 2 should raise a red flag that we'll likely end up with heteroscedastic errors (aka our errors will get worse the longer the snail).  Let's prove this out.\n",
    "\n",
    "Below we see that our model fits the data as best as it can.  By using transparency we can see that the line might be a little bit biased towards overpredicting (i.e. the most dense/opaque clumps of points are below the line).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone[\"Length\"]\n",
    "y = abalone[\"Rings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=40\n",
    ")\n",
    "\n",
    "# Add a constant for the intercept\n",
    "# This will work but throw a warning.. to silence the warning you can pass X.values instead\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_train)\n",
    "\n",
    "lm_results = sm.OLS(y_train, X_train_const).fit()\n",
    "y_pred = lm_results.predict(X_test_const)\n",
    "\n",
    "plt.scatter(X_train, y_train, alpha=0.1)\n",
    "plt.plot(X_train, y_pred, c=\"red\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Rings\")\n",
    "plt.show()\n",
    "\n",
    "lm_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some residual plots, which is where we actually diagnose problems with homoscedasticity.  We look at residuals because we'll typically be working in higher dimensions than 2d (which is the only reason we were able to spot this problem this early).\n",
    "\n",
    "We'll create a random normal sample of residuals to compare our observed residuals against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_residuals = lm_results.resid\n",
    "rand_norm_residuals = np.random.normal(0, 3, len(y_train))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].scatter(X_train, rand_norm_residuals, alpha=0.2)\n",
    "axes[0].axhline(0, c=\"red\", alpha=0.5)\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Residual\")\n",
    "axes[0].set_title(\"What we want to see\")\n",
    "\n",
    "axes[1].scatter(X_train, true_residuals, alpha=0.2)\n",
    "axes[1].axhline(0, c=\"red\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "axes[1].set_ylabel(\"Residual\")\n",
    "axes[1].set_title(\"What we actually see\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further prove this out with a formal test if we're not convinced.  The reading shows bartlett & levene tests for this.  These test are more generic tests of variance, and in the case of checking this assumption, they are pretty sensitive and will say you're violating the assumption with slight deviations.\n",
    "\n",
    "I recommend something more directly designed for testing this assumption.  This is a good ole hypothesis test, so we'll get a null, alternative, and a p-value.\n",
    "\n",
    "* $H_0$: Data is homoscedastic\n",
    "* $H_a$: Data is heteroscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hypothesis: data is homoscedastic\n",
    "_, p, _, _ = het_breuschpagan(true_residuals, X_train_const)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... so we have a problem, we violated the homoscedasticiticity assumption. Now what... well as always, it depends.\n",
    "\n",
    "Hopefully you're a domain expert on the data your modeling or your working with one.  Maybe there's a variable your snail scientist knows about that can lead to higher ring counts.  For example, maybe a snail's diet (like iron intake?) is a good predictor of rings and including some features about diet will capture this pattern of higher variation in longer snails.\n",
    "\n",
    "An alternative is to try out a 'variance stabilizing transformation'.  Our whole issue is around variance so we might be able to take tackle this head on. This can might be done with `sklearn.preprocessing.power_transform()`.  In this case, it doesn't solve the issue, and is omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity\n",
    "\n",
    "This is when you have redundancy in your predictors.  If you have highly redundant features (i.e. celsius and fahrenheit or dummy variables without dropping one) this is a really big issue for the model.  The model can't figure out which of the corelated variables to use.\n",
    "\n",
    "For example, if you have salt and sugar unlabeled in the kitchen, and you asked people to make a recipe (without tasting), you'd expect people to mess up which is which.  They look similar (ie highly correlated), so people ended up using the wrong amounts of each one.  This is kind of similar to the model's issues with highly correlated variables, it doesn't know which to use and might use the wrong amount of each one (i.e. it doesn't know how to assign the right coefficient).  If the coefficients are 'unstable' we can say they have a high 'variance'.\n",
    "\n",
    "This is how we can directly identify multicolinearity, by looking at the variance of the coeficients using the 'variance inflation factor'.  Variables with a high *VIF* are highly redundant of another variable (similar to correlations).\n",
    "\n",
    "Note, correlations can also help identify this issue, VIF just gives us a single number per feature so theres less to theres numbers to have to look through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variance inflation factors range from 1 upwards. The numerical value for VIF tells you (in decimal form) what percentage the variance (i.e. the standard error squared) is inflated for each coefficient. For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity — if there was no correlation with other predictors.\n",
    "A rule of thumb for interpreting the variance inflation factor:\n",
    ">\n",
    ">  * 1 = not correlated.\n",
    ">  * Between 1 and 5 = moderately correlated.\n",
    ">  * Greater than 5 = highly correlated.\n",
    ">\n",
    "> Exactly how large a VIF has to be before it causes issues is a subject of debate. What is known is that the more your VIF increases, the less reliable your regression results are going to be. In general, a VIF above 10 indicates high correlation and is cause for concern. Some authors suggest a more conservative level of 2.5 or above.\n",
    "\n",
    "Source: [statisticshowto](https://www.statisticshowto.com/variance-inflation-factor/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy/paste a VIF function from here:\n",
    "# https://gist.github.com/AdamSpannbauer/c99c366b0c7d5b6c4920a46c32d738e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = abalone[[\"Rings\", \"Shucked_Weight\", \"Height\", \"Diameter\"]]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "vifs = []\n",
    "for i in range(X.shape[1]):\n",
    "    vif = variance_inflation_factor(X.values, i)\n",
    "    vifs.append(vif)\n",
    "\n",
    "pd.Series(vifs, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we forget to drop a level of a dummy variable we have perfectly redundant information in the inputs.\n",
    "\n",
    "Given any 2 we can caluclate the other: `I = 1 - M + F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(abalone[\"Sex\"], drop_first=False)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF will reflect that theres a big issue of multicollinearity in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "\n",
    "vifs = []\n",
    "for i in range(X.shape[1]):\n",
    "    vif = variance_inflation_factor(X.values, i)\n",
    "    vifs.append(vif)\n",
    "\n",
    "pd.Series(vifs, index=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
