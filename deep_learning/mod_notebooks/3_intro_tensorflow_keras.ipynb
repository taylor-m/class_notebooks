{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow and Keras\n",
    "\n",
    "Before starting the installation, it is important to note the computation units that TensorFlow could use on your machine. You have two options for running your TensorFlow code: you can use the central processing unit (CPU), or you can use the graphics processing unit (GPU).\n",
    "\n",
    "Because GPUs can run linear matrix operations faster than CPUs, data scientists usually prefer to use GPUs when they're available. But although training deep-learning models on GPUs is way faster than training them on CPUs, accessing GPUs can be costly. The easiest solution for training your models on a GPU is to use Google Colaboratory, which offers free GPUs. That being said, the TensorFlow code that you'll write will be the same regardless of the underlying computational unit.\n",
    "\n",
    "Now, start with the installation of TensorFlow. In doing so, you can make use of Python's pip package manager. If you'd like to install the CPU version of TensorFlow, then just run this command on your terminal:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.0.0-rc1\n",
    "```\n",
    "\n",
    "However, if you'd like to use GPU version of TensorFlow, you should run the following:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow-gpu==2.0.0-rc1\n",
    "```\n",
    "\n",
    "When you install TensorFlow, Keras comes bundled with it. So, you don't need to install Keras separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Keras\n",
    "\n",
    "To define and run a deep-learning model, Keras offers three different options:\n",
    "\n",
    "* The sequential API\n",
    "* The functional API\n",
    "* Model subclassing\n",
    "\n",
    "Throughout this module, you'll make use of the sequential API because it offers the easiest way to define and run a deep-learning model. With the other two methods, you could write some more sophisticated deep-learning architectures. But for the purposes of this module, the sequential API offers more than enough. You can import it in your code as follows:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential \n",
    "```\n",
    "\n",
    "As you'll see shortly, you'll build your ANN step by step, using the `Sequential()` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a basic ANN model\n",
    "\n",
    "It's quite easy to build a deep-learning model using the sequential API from Keras. Remember that deep-learning models consist of layers stacked on top of each other; for example, the second layer comes after the first layer, the third layer comes after the second layer, and so on. Keras mimics this sequential structure of the deep-learning model when implementing them programmatically.\n",
    "\n",
    "When implementing an ANN in Keras, you use the following steps:\n",
    "\n",
    "1. First, create a `model` object.\n",
    "2. Second, add layers to the model one by one.\n",
    "\n",
    "After doing these steps, you'll end up with a deep-learning model structure. The next steps are as follows:\n",
    "\n",
    "1. Define an optimizer and compile the model.\n",
    "2. After compiling the model, train the model using training data.\n",
    "3. Evaluate the performance of the model on a test set.\n",
    "\n",
    "The figure below sketches the steps for implementing a deep-learning model in Keras:\n",
    "\n",
    "![keras](assets/keras.png)\n",
    "\n",
    "That's all! Next, get familiar with the dataset that you'll be using in this checkpoint. Then, you'll implement your model using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Throughout this module, you'll be using a famous optical character recognition dataset called *MNIST*. This dataset consists of 70,000 grayscale images of handwritten digits. *MNIST* is a classic dataset for deep-learning research and education, and that's why it was chosen for this module. In the following, you'll load the dataset and do some data preprocessing. As you'll see shortly, each image is represented as 28x28 pixel data. This is a two-dimensional vector. You'll convert this to a vector of length 784, which will be single-dimensional. You'll also normalize each vector by dividing each element by 255 (which is the maximum value of the RGB color scale). \n",
    "\n",
    "Load the *MNIST* dataset using the `mnist` class from the `keras.datasets` module. In order to do that, import it as shown below.\n",
    "\n",
    "**Note:** You don't have to use this method to download the *MNIST* dataset. It's [available online](http://yann.lecun.com/exdb/mnist/). You can also download the dataset from that link and then load it your own way. Notice that the dataset in the link is separated into two (as training and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d5c617a37f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the data and do the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, one-hot code your target variable using the `to_categorical()` function from the `keras.utils` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the size of each image is 784. In fact, all images in *MNIST* are 28 by 28 pixels, and 784 is just the result of multiplying 28 by 28. So, the data that you have is a flattened version of the images, where each row in the 28x28 matrix is concatenated side by side. \n",
    "\n",
    "Now, plot some images and see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(X_train[123].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[123]))\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(X_train[124].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[124]))\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(X_train[125].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[125]))\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(X_train[126].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[126]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now ready to jump into building the ANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "As mentioned before, you'll build your model using the `Sequential()` class of the `keras.models` module. First, create your model as follows:\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "Then you can start to add layers to your `model` object one by one (that is, sequentially). The layer type that you'll use is called the dense layer, and you can import it from the `keras.layers` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# The first dense layer\n",
    "model.add(Dense(1028, input_shape=(784,), activation=\"relu\"))\n",
    "# The second dense layer\n",
    "model.add(Dense(1028, activation=\"relu\"))\n",
    "# The last layer is the output layer\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, the neuron size of the output layer is set to `10`. This is because *MNIST* has 10 classes. The code also sets the activation function of the output layer to `softmax`. Later, you'll explore why softmax is used as the activation function in the output layer. But for now, know that when you give an image as an input to the model, your model will produce 10 probabilities for each of the 10 classes in the *MNIST* data. The largest probability class will be the prediction of the model.\n",
    "\n",
    "You can look at the structure of your ANN model using the `summary()` method of your `model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have three dense layers, and the last one is the output layer. In total, there are 1,875,082 parameters to be estimated in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now you can compile your model. When compiling the model, define three things:\n",
    "\n",
    "1. **The optimizer that will be used in the training:** If you don't know about optimizers in deep learning, don't worry. You just need to use one in this checkpoint, and you'll learn about optimizers later in this module.\n",
    "2. **The loss function:** It's necessary to specify a loss function for a model. Training algorithms use this loss function and try to minimize it during the training. This will also be covered in the next checkpoint.\n",
    "3. **The metric to measure the training performance of your model:** In this example, you use the accuracy metric, because your task is a classification task and your dataset is a balanced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "You're now ready to train the model. Training a model in Keras is done by calling the `fit()` method of the `model` object. In the following, you train your model using the following specifications:\n",
    "\n",
    "* Use `128` as the batch size. *Batch size* is something that you'll explore in a later checkpoint.\n",
    "* Use `20` as the number of epochs. In deep-learning jargon, *epoch* means full use of all of the examples in the training data during the training of the model. So, you'll train your model during 20 epochs, which means that you'll use all of the observations in your training data 20 times when training your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting `verbose=1` prints out some results after each epoch\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The model achieved almost 97% accuracy in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "\n",
    "The last step is to evaluate the model using the test set that you set apart before. For this purpose, use the `evaluate()` method of the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy of the model is almost 97%. Good job! Now it's your turn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
